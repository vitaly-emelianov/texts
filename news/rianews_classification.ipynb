{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##<center>Классификация статей ria.ru по рубрикам</center>\n",
    "<center>Емельянов Виталий, ФИВТ</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Извлечение данных\n",
    "В качестве источника обучающей выборки используем сайт РИА-Новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "\n",
    "rianews = newspaper.build(\"http://ria.ru/\", memoize_articles=False, language='ru')\n",
    "print rianews.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как библиотека newspaper справляется с парсингом статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ria.ru/incidents/20150206/1046276722.html\n",
      "\n",
      "МОСКВА, 6 фев — РИА Новости. Правоохранители обнаружили в общежитии одного из московских ВУЗов тело убитого студента, сообщило столичное управление СК РФ в пятницу.\n",
      "\n",
      "\"По данным следствия, 5 февраля 2015 года, примерно в 5 часов 10 минут, в общежитии одного из институтов по Измайловскому проспекту в городе Москве обнаружено тело студента 1993 года рождения с признаками насильственной смерти в виде колото-резанных ранений в области головы и шеи\", — говорится в сообщении.\n",
      "\n",
      "Проводится проверка, возбуждено дело об убийстве.\n",
      "\n",
      "В каком именно институте учился молодой человек, не уточняется, но, по данным СМИ, речь идет об МГТУ имени Баумана.\n",
      "\n",
      "Ранее в ряде СМИ сообщалось, что прошлой ночью около общежития МГТУ им. Баумана на востоке Москвы студент заступился за девушку, к которой пристали неизвестные. Они нанесли студенту несколько ножевых ранений, после чего скрылись, молодой человек скончался на месте. По другой версии, студент сам начал конфликт.\n"
     ]
    }
   ],
   "source": [
    "first_article = rianews.articles[5]\n",
    "first_article.download()\n",
    "first_article.parse()\n",
    "print first_article.url\n",
    "print\n",
    "print first_article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выкачаем первые 990 статей, извлечем из каждой текст и сохраним в папке ria/ в отдельный файл %N.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in xrange(990):\n",
    "    rianews.articles[i].download()\n",
    "    rianews.articles[i].parse()\n",
    "    filename = \"ria/\" + str(counter) + \".txt\"\n",
    "    counter += 1\n",
    "    with open(filename, 'w+') as f:\n",
    "        f.write(rianews.articles[i].url)\n",
    "        f.write('\\n')\n",
    "        f.write(rianews.articles[i].text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Построение обучающей выборки\n",
    "\n",
    "Посмотрим на структуру url сайта ria.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ria.ru/culture/20151115/1101521140.html\n",
      "http://ria.ru/tv_sport/20151113/1320087183.html\n",
      "http://ria.ru/economy/20151115/1321054488.html#comments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open('ria/1.txt', 'r') as f:\n",
    "    data_in = f.read()\n",
    "lines = data_in.split('\\n')\n",
    "url = lines[0]  \n",
    "print url\n",
    "\n",
    "with open('ria/132.txt', 'r') as f:\n",
    "    data_in = f.read()\n",
    "lines = data_in.split('\\n')\n",
    "url = lines[0]  \n",
    "print url\n",
    "\n",
    "with open('ria/493.txt', 'r') as f:\n",
    "    data_in = f.read()\n",
    "lines = data_in.split('\\n')\n",
    "url = lines[0]  \n",
    "print url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно предположить, что рубрика статьи - это то, что идет после \"http://ria.ru/\" до  первого \"/\".\n",
    "Извлечем из выкачанного множества статей, рубрику и текст статьи, исходя из нашего предположения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 498\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urlparse\n",
    "\n",
    "\n",
    "file_list = os.listdir('./ria')\n",
    "\n",
    "news_list = []\n",
    "for filename in file_list:\n",
    "    with open('ria/' + filename, 'r') as f:\n",
    "        data_in = f.read()\n",
    "        lines = data_in.split('\\n')\n",
    "        url = lines[0]\n",
    "        topic = urlparse.urlsplit(url).path.split('/')[1]\n",
    "        text = '\\n'.join(lines[1:])\n",
    "        news_list.append({'topic': topic, 'text': text.decode('utf-8')})\n",
    "print \"Number of articles:\", len(news_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заведем set из рубрик и посмотрим на них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 45\n",
      "\n",
      "Topics:\n",
      "Tech_news 1\n",
      "analytics 7\n",
      "announce 74\n",
      "arab_sy 1\n",
      "asia 3\n",
      "culture 1\n",
      "docs 1\n",
      "economy 82\n",
      "games 2\n",
      "gl 3\n",
      "history_video 3\n",
      "incidents 7\n",
      "infografika 1\n",
      "moscow 6\n",
      "multimedia 2\n",
      "opinions 1\n",
      "photolents 3\n",
      "politics 74\n",
      "r_chronicles 1\n",
      "religion 1\n",
      "science 2\n",
      "sochi2014_torchrelay 1\n",
      "society 6\n",
      "syria 1\n",
      "syria_is 1\n",
      "syria_mm 4\n",
      "tags 3\n",
      "titanic_interactive 1\n",
      "tournament 17\n",
      "trend 2\n",
      "tv_culture 1\n",
      "tv_defense_safety 3\n",
      "tv_eco 3\n",
      "tv_economy 1\n",
      "tv_incidents 9\n",
      "tv_politics 20\n",
      "tv_science 2\n",
      "tv_society 12\n",
      "tv_sport 5\n",
      "valdaiclub_anniversary_announcement 1\n",
      "victorina 4\n",
      "video 8\n",
      "videoclub 3\n",
      "vote 3\n",
      "world 111\n"
     ]
    }
   ],
   "source": [
    "topics = dict()\n",
    "for i in xrange(len(news_list)):\n",
    "    if news_list[i]['topic'] not in topics:\n",
    "       topics[news_list[i]['topic']] = 1\n",
    "    else:\n",
    "        topics[news_list[i]['topic']] += 1\n",
    "  \n",
    "print \"Number of topics:\", len(topics)\n",
    "print\n",
    "print \"Topics:\"\n",
    "\n",
    "for item in sorted(topics.items()):\n",
    "    print item[0], item[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что наше предположение оказалось верным и у нас 45 рубрик."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суффикс tv у рубрики говорит о том, что в статье находится видеоролик как это можно видеть на примере http://ria.ru/tv_culture/20151111/1318661856.html<br/>\n",
    "Будем считать, что tv_rubric то же, что и rubric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for news in news_list:\n",
    "    if news['topic'] == \"tv_culture\":\n",
    "        news['topic'] = \"culture\"\n",
    "    if news['topic'] == \"tv_economy\":\n",
    "        news['topic'] = \"economy\"\n",
    "    if news['topic'] == \"tv_incidents\":\n",
    "        news['topic'] = \"incidents\"\n",
    "    if news['topic'] == \"tv_politics\":\n",
    "        news['topic'] = \"politics\"\n",
    "    if news['topic'] == \"tv_society\":\n",
    "        news['topic'] = \"society\"\n",
    "    if news['topic'] == \"tv_sport\":\n",
    "        news['topic'] = \"sport\"\n",
    "    if news['topic'] == \"tv_science\":\n",
    "        news['topic'] = \"science\"\n",
    "    if news['topic'] == \"tv_defense_safety\":\n",
    "        news['topic'] = \"defense_safety\"\n",
    "    if news['topic'] == \"tv_eco\":\n",
    "        news['topic'] = \"eco\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также мы видим что есть 3 рубрики по Сирии: syria, syria_is, syria_mm.<br/>\n",
    "Видимо, syria_is - все что касается ISIS, а syria_mm - military map. Объединим эти три рубрики в одну."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tech_news 1\n",
      "analytics 7\n",
      "announce 74\n",
      "arab_sy 1\n",
      "asia 3\n",
      "culture 2\n",
      "defense_safety 3\n",
      "docs 1\n",
      "eco 3\n",
      "economy 83\n",
      "games 2\n",
      "gl 3\n",
      "history_video 3\n",
      "incidents 16\n",
      "infografika 1\n",
      "moscow 6\n",
      "multimedia 2\n",
      "opinions 1\n",
      "photolents 3\n",
      "politics 94\n",
      "r_chronicles 1\n",
      "religion 1\n",
      "science 4\n",
      "sochi2014_torchrelay 1\n",
      "society 18\n",
      "sport 5\n",
      "syria 6\n",
      "tags 3\n",
      "titanic_interactive 1\n",
      "tournament 17\n",
      "trend 2\n",
      "valdaiclub_anniversary_announcement 1\n",
      "victorina 4\n",
      "video 8\n",
      "videoclub 3\n",
      "vote 3\n",
      "world 111\n"
     ]
    }
   ],
   "source": [
    "for news in news_list:\n",
    "    if news['topic'] == \"syria_is\":\n",
    "        news['topic'] = \"syria\"\n",
    "    if news['topic'] == \"syria_mm\":\n",
    "        news['topic'] = \"syria\"\n",
    "        \n",
    "topics = dict()\n",
    "for i in xrange(len(news_list)):\n",
    "    if news_list[i]['topic'] not in topics:\n",
    "       topics[news_list[i]['topic']] = 1\n",
    "    else:\n",
    "        topics[news_list[i]['topic']] += 1\n",
    "for item in sorted(topics.items()):\n",
    "    print item[0], item[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исключим статьи и рубрики, где меньше 4 статей в рубрике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytics 7\n",
      "announce 74\n",
      "economy 83\n",
      "incidents 16\n",
      "moscow 6\n",
      "politics 94\n",
      "science 4\n",
      "society 18\n",
      "sport 5\n",
      "syria 6\n",
      "tournament 17\n",
      "victorina 4\n",
      "video 8\n",
      "world 111\n"
     ]
    }
   ],
   "source": [
    "min_size = 4\n",
    "\n",
    "updated_news_list = []\n",
    "updated_topics = dict()\n",
    "for topic in topics:\n",
    "    if topics[topic] >= min_size:\n",
    "        updated_topics[topic] = topics[topic]\n",
    "        for article in news_list:\n",
    "            if article['topic'] == topic:\n",
    "                updated_news_list.append({'topic': article['topic'], 'text': article['text']})\n",
    "news_list = updated_news_list\n",
    "topics = updated_topics\n",
    "\n",
    "for item in sorted(topics.items()):\n",
    "    print item[0], item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles:  453\n"
     ]
    }
   ],
   "source": [
    "print \"Number of articles: \", len(news_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть размеченная выборка размером в 453 статьи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3. Построение признаков и baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для построения baseline постороим bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = []\n",
    "answers = []\n",
    "for article in news_list:\n",
    "    answers.append(article['topic'])\n",
    "    texts.append(article['text'])\n",
    " \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим какие-нибудь алгоритмы классификации к нашему bag-of-words, посмотрим на качество и выберем в качестве baseline алгоритм с лучшим качеством."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality:  0.687415595575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "print \"Quality: \", np.mean(cross_val_score(nb_classifier, X, np.array(answers), scoring=\"f1_weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality:  0.700818312857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators = 60)\n",
    "print \"Quality: \", np.mean(cross_val_score(random_forest_classifier, X, np.array(answers), scoring=\"f1_weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality:  0.759952584239\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_classifier = LogisticRegression()\n",
    "print \"Quality: \", np.mean(cross_val_score(logistic_classifier, X, np.array(answers), scoring=\"f1_weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия показала самое лучшее качество. Посмотрим как она классифицирует следующую статью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  politics\n"
     ]
    }
   ],
   "source": [
    "logistic_classifier.fit(X, answers)\n",
    "ans = logistic_classifier.predict(vectorizer.transform([\"Выход фильма «Сделано во Франции» о подготовке теракта в Париже отложили на неопределенное время, сообщает The Time. Такое решение кинопрокатчики приняли на фоне террористических атак, которые произошли во французской столице в ночь на 14 ноября.\"]))\n",
    "print \"Answer: \", ans[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что это больше походит на анонс нежели на статью о политике. Хотя кто-то, думаю, может отнести ее и к политической новости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###4. Более хитрые признаки "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве признаков добавим еще 2-, 3-, 4-граммы и посмотрим на качество классификации логистической регрессией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality:  0.759346971953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 4), min_df=1) \n",
    "X = ngram_vectorizer.fit_transform(texts) \n",
    "\n",
    "from sklearn import linear_model \n",
    "\n",
    "\n",
    "logistic_classifier = LogisticRegression()\n",
    "print \"Quality: \", np.mean(cross_val_score(logistic_classifier, X, np.array(answers), scoring=\"f1_weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Видим, что качество осталось тем же, что и при использовании bag-of-words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
